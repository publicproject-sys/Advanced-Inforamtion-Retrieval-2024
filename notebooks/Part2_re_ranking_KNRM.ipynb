{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Neural Re-Ranking **40 points**\n",
    "\n",
    "Implement 2 neural architectures based on the kernel-pooling paradigm to perform re-ranking in ``src/re_ranking.py`` (KNRM, TK)\n",
    "\n",
    "- Implement: the 2 (KNRM, TK) model classes **20 points**\n",
    "   - Show that you understood what happens by adding comments to difficult parts of the model (what tensor dimensions represent, what gets summed up, etc..)\n",
    "- Implement: training process & result evaluation **10 points**\n",
    "    - Including early stopping based on the validation set\n",
    "\t   - Use the **msmarco_tuples.validation.tsv** input to feed the neural models and **msmarco_qrels.txt** qrels to evaluate the output\n",
    "- Evaluate: Compute a test set evaluation at the end  **10 points**\n",
    "\t- MS-MARCO sparse labels\n",
    "\t  - Use the **msmarco_tuples.test.tsv** input to feed the neural models and **msmarco_qrels.txt** qrels to evaluate the output\n",
    "\t- FiRA-2022 fine-grained labels on out-of-domain data\n",
    "\t  - Use your created created labels from part 1\n",
    "\t     - Use the **fira-2022.tuples.tsv** input to feed the neural models and your qrels from part 1 to evaluate the output\n",
    "\t  - Compare these results with our baseline label creation method\n",
    "\t     - Use the **fira-2022.tuples.tsv** input to feed the neural models and **fira-2022.baseline-qrels.tsv** qrels to evaluate the output\n",
    "\t  - Explore & describe the differences in metrics between the baseline and your label creation \n",
    "\n",
    "## Provided data:\n",
    "* AllenNLP vocabulary (collection specific, in two sizes: use the _10 = min of 10 occurrences in the collection if you have memory problems with the _5)\n",
    "* train triples\n",
    "* evaluation tuples (validation & test) with 2.000 queries each and the top 40 BM25 results per query, relevance judgments (qrels, one file covering both validation & test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Put in your basepath like for example \"/home/studio-lab-user/src/data_part2\"\n",
    "base_path = \"../\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Iterator, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data_loading import IrTripleDatasetReader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "\n",
    "from src.data_loading import *\n",
    "from allennlp.data.dataloader import PyTorchDataLoader\n",
    "import pandas as pd\n",
    "from src.model_knrm import *\n",
    "from src.BatchWordEmbedder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocab_directory\": os.path.join(base_path, \"data/Part-2/allen_vocab_lower_10\"),\n",
    "    \"pre_trained_embedding\": os.path.join(base_path, \"data/Part-2/glove.42B.300d.txt\"),\n",
    "    \"model\": \"knrm\",\n",
    "    \"train_data\": os.path.join(base_path, \"data/Part-2/triples.train.tsv\"),\n",
    "    \"validation_data\": os.path.join(base_path, \"data/Part-2/msmarco_tuples.validation.tsv\"),\n",
    "    \"test_data\": os.path.join(base_path, \"data/Part-2/tuples.test.tsv\"),\n",
    "    \"qrels\": os.path.join(base_path, \"data/Part-2/msmarco_qrels.txt\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223981c9ebd84f888a6c6ddf424595af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = Vocabulary.from_files(config[\"vocab_directory\"])\n",
    "tokens_embedder = Embedding(vocab=vocab,\n",
    "                           pretrained_file= config[\"pre_trained_embedding\"],\n",
    "                           embedding_dim=300,\n",
    "                           trainable=True,\n",
    "                           padding_index=0,\n",
    "                           )\n",
    "word_embedder = BasicTextFieldEmbedder({\"tokens\": tokens_embedder})\n",
    "\n",
    "_triple_reader = IrTripleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n",
    "_triple_reader = _triple_reader.read(config[\"train_data\"])\n",
    "_triple_reader.index_with(vocab)\n",
    "loader = PyTorchDataLoader(_triple_reader, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "batch_embedder = BatchWordEmbedder(word_embedder, device)\n",
    "model_knrm = KNRM(n_kernels=11).to(device)\n",
    "optimizer = torch.optim.Adam(model_knrm.parameters(), lr=0.5 * 1e-3)\n",
    "loss_criterion = torch.nn.MarginRankingLoss(margin=1, reduction='elementwise_mean').to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94479cc7c2b84a3c89f9a7c69d67fb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/airenv/lib/python3.6/site-packages/torch/nn/_reduction.py:14: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch loss: 0.9861485958099365\n",
      "50 Batch loss: 0.9711251854896545\n",
      "100 Batch loss: 0.9416072964668274\n",
      "150 Batch loss: 0.9400886297225952\n",
      "200 Batch loss: 0.9375748634338379\n",
      "250 Batch loss: 0.9009732007980347\n",
      "300 Batch loss: 0.9000574350357056\n",
      "350 Batch loss: 0.8765318393707275\n",
      "400 Batch loss: 0.8214479684829712\n",
      "450 Batch loss: 0.8288471102714539\n",
      "500 Batch loss: 0.8129605650901794\n",
      "550 Batch loss: 0.7374101877212524\n",
      "600 Batch loss: 0.7331011295318604\n",
      "650 Batch loss: 0.7029364109039307\n",
      "700 Batch loss: 0.627629280090332\n",
      "750 Batch loss: 0.7026369571685791\n",
      "800 Batch loss: 0.624007523059845\n",
      "850 Batch loss: 0.6411666870117188\n",
      "900 Batch loss: 0.507813036441803\n",
      "950 Batch loss: 0.6552830934524536\n",
      "1000 Batch loss: 0.579552173614502\n",
      "1050 Batch loss: 0.5931665897369385\n",
      "1100 Batch loss: 0.5118998289108276\n",
      "1150 Batch loss: 0.5565185546875\n",
      "1200 Batch loss: 0.5134824514389038\n",
      "1250 Batch loss: 0.5800173282623291\n",
      "1300 Batch loss: 0.5316937565803528\n",
      "1350 Batch loss: 0.542830228805542\n",
      "1400 Batch loss: 0.4688374698162079\n",
      "1450 Batch loss: 0.5676379203796387\n",
      "1500 Batch loss: 0.5166724920272827\n",
      "1550 Batch loss: 0.49125760793685913\n",
      "1600 Batch loss: 0.5213342308998108\n",
      "1650 Batch loss: 0.49594858288764954\n",
      "1700 Batch loss: 0.46624118089675903\n",
      "1750 Batch loss: 0.4453970789909363\n",
      "1800 Batch loss: 0.4953698515892029\n",
      "1850 Batch loss: 0.49585703015327454\n",
      "1900 Batch loss: 0.4153338670730591\n",
      "1950 Batch loss: 0.4194489121437073\n",
      "2000 Batch loss: 0.487076997756958\n",
      "2050 Batch loss: 0.43220022320747375\n",
      "2100 Batch loss: 0.49995654821395874\n",
      "2150 Batch loss: 0.466410368680954\n",
      "2200 Batch loss: 0.4276387691497803\n",
      "2250 Batch loss: 0.46184641122817993\n",
      "2300 Batch loss: 0.42596161365509033\n",
      "2350 Batch loss: 0.42603209614753723\n",
      "2400 Batch loss: 0.44537848234176636\n",
      "2450 Batch loss: 0.5401105880737305\n",
      "2500 Batch loss: 0.38080817461013794\n",
      "2550 Batch loss: 0.48716598749160767\n",
      "2600 Batch loss: 0.35477691888809204\n",
      "2650 Batch loss: 0.4012227952480316\n",
      "2700 Batch loss: 0.384919136762619\n",
      "2750 Batch loss: 0.39811021089553833\n",
      "2800 Batch loss: 0.3858206868171692\n",
      "2850 Batch loss: 0.4348018765449524\n",
      "2900 Batch loss: 0.40077903866767883\n",
      "2950 Batch loss: 0.44773176312446594\n",
      "3000 Batch loss: 0.408349871635437\n",
      "3050 Batch loss: 0.37543994188308716\n",
      "3100 Batch loss: 0.35135871171951294\n",
      "3150 Batch loss: 0.45924609899520874\n",
      "3200 Batch loss: 0.3744558095932007\n",
      "3250 Batch loss: 0.35298851132392883\n",
      "3300 Batch loss: 0.3668839633464813\n",
      "3350 Batch loss: 0.3968512713909149\n",
      "3400 Batch loss: 0.4821512699127197\n",
      "3450 Batch loss: 0.37153780460357666\n",
      "3500 Batch loss: 0.375796914100647\n",
      "3550 Batch loss: 0.34825053811073303\n",
      "3600 Batch loss: 0.35367023944854736\n",
      "3650 Batch loss: 0.42669764161109924\n",
      "3700 Batch loss: 0.3995940685272217\n",
      "3750 Batch loss: 0.37485817074775696\n",
      "3800 Batch loss: 0.36957240104675293\n",
      "3850 Batch loss: 0.36497920751571655\n",
      "3900 Batch loss: 0.38861319422721863\n"
     ]
    }
   ],
   "source": [
    "model_knrm_trained = knrm_training_loop(model_knrm, loader, optimizer, loss_criterion, batch_embedder, device, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input path of the saved model\n",
    "model_path = os.path.join(base_path, \"models/model_knrm_trained.pth\")\n",
    "\n",
    "# Define the configuration for TK model initialization\n",
    "knrm_config = {\n",
    "    \"n_kernels\": 11,\n",
    "}\n",
    "\n",
    "# Initialize the TK model\n",
    "model_knrm = KNRM(**knrm_config).to(device)\n",
    "\n",
    "# Load the model state_dict\n",
    "model_knrm.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**MS Marco Sparse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read txt file\n",
    "df_qrels = pd.DataFrame()\n",
    "list_qrels = []\n",
    "with open(config['qrels']) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        query_id, _, doc_id, _ = line.split()\n",
    "        list_qrels.append([query_id, doc_id])\n",
    "df_qrels = pd.DataFrame(list_qrels, columns=['query_id', 'doc_id'])\n",
    "df_qrels['query_id'] = df_qrels['query_id'].astype(int)\n",
    "df_qrels['doc_id'] = df_qrels['doc_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "batch_embedder_eval = BatchWordEmbedder(word_embedder, device, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_triple_reader_eval = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=128, max_query_length=128)\n",
    "_triple_reader_eval = _triple_reader_eval.read(config[\"validation_data\"])\n",
    "_triple_reader_eval.index_with(vocab)\n",
    "loader_test = PyTorchDataLoader(_triple_reader_eval, batch_size=64)\n",
    "# batch = next(iter(loader_test))\n",
    "# batch_embedder_eval = BatchWordEmbedder(word_embedder, device, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, df_qrels, loader, batch_embedder, device):\n",
    "    model.eval()\n",
    "    query_ids = []\n",
    "    doc_ids = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(loader):\n",
    "            query_emb, doc_pos_emb, _, query_pad_mask, document_pad_mask_pos, _ = batch_embedder(batch)\n",
    "            pred = model(query_emb, doc_pos_emb, query_pad_mask, document_pad_mask_pos)\n",
    "            query_ids.extend(batch['query_id'])  # Directly extend the list\n",
    "            doc_ids.extend(batch['doc_id'])      # Directly extend the list\n",
    "            preds.extend(pred.cpu().numpy().flatten())\n",
    "            # if idx * 32 >= 10000:\n",
    "            #     break\n",
    "    df_eval = pd.DataFrame({\n",
    "        'query_id': query_ids,\n",
    "        'doc_id': doc_ids,\n",
    "        'score': preds\n",
    "    })\n",
    "    \n",
    "    # Assigning rank based on scores within each query_id group\n",
    "    df_eval['rank'] = df_eval.groupby('query_id')['score'].rank(ascending=False, method='first').astype(int)\n",
    "    \n",
    "    # Sorting by query_id and rank\n",
    "    df_eval = df_eval.sort_values(by=['query_id', 'rank'])\n",
    "    \n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2542d968e74cb78ac2c1bbd0877211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_eval = evaluate_model(model_knrm, df_qrels, loader_test, batch_embedder_eval, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the specific paths relative to the base path\n",
    "path_result = os.path.join(base_path, \"data/results_part2/knrm_msmarco_ranking_final.tsv\")\n",
    "path_baseline = config['qrels']\n",
    "\n",
    "\n",
    "# Write the DataFrame to a TSV file\n",
    "df_eval.to_csv(path_result, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR@10': 0.1817690476190476,\n",
       " 'Recall@10': 0.369375,\n",
       " 'QueriesWithNoRelevant@10': 1245,\n",
       " 'QueriesWithRelevant@10': 755,\n",
       " 'AverageRankGoldLabel@10': 3.7072847682119203,\n",
       " 'MedianRankGoldLabel@10': 3.0,\n",
       " 'MRR@20': 0.18971659236875957,\n",
       " 'Recall@20': 0.47979166666666667,\n",
       " 'QueriesWithNoRelevant@20': 1019,\n",
       " 'QueriesWithRelevant@20': 981,\n",
       " 'AverageRankGoldLabel@20': 6.243628950050969,\n",
       " 'MedianRankGoldLabel@20': 4.0,\n",
       " 'MRR@1000': 0.19311101499504907,\n",
       " 'Recall@1000': 0.574625,\n",
       " 'QueriesWithNoRelevant@1000': 830,\n",
       " 'QueriesWithRelevant@1000': 1170,\n",
       " 'AverageRankGoldLabel@1000': 9.876068376068377,\n",
       " 'MedianRankGoldLabel@1000': 6.0,\n",
       " 'nDCG@3': 0.16749706819748705,\n",
       " 'nDCG@5': 0.1919873251456052,\n",
       " 'nDCG@10': 0.22463290914396003,\n",
       " 'nDCG@20': 0.2531676692056566,\n",
       " 'nDCG@1000': 0.272969811529321,\n",
       " 'QueriesRanked': 2000,\n",
       " 'MAP@1000': 0.18992419478055275}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.core_metrics import calculate_metrics_plain,load_ranking,load_qrels\n",
    "\n",
    "calculate_metrics_plain(load_ranking(path_result),load_qrels(path_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**fira-2022.tuples.tsv input to feed the neural models and fira-2022.baseline-qrels.tsv for evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    \"vocab_directory\": os.path.join(base_path, \"data/Part-2/allen_vocab_lower_10\"),\n",
    "    \"pre_trained_embedding\": os.path.join(base_path, \"data/Part-2/glove.42B.300d.txt\"),\n",
    "    \"model\": \"knrm\",\n",
    "    \"train_data\": os.path.join(base_path, \"data/Part-2/triples.train.tsv\"),\n",
    "    \"validation_data\": os.path.join(base_path, \"data/Part-2/fira-22.tuples_mod.tsv\"),\n",
    "    \"test_data\": os.path.join(base_path, \"data/Part-2/tuples.test.tsv\"),\n",
    "    \"qrels\": os.path.join(base_path, \"data/Part-2/fira-22.baseline-qrels.tsv\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path to your file\n",
    "\n",
    "# Initialize an empty list to store [query_id, doc_id] pairs\n",
    "list_qrels = []\n",
    "\n",
    "# Read the file line by line and process each line\n",
    "with open(config['qrels']) as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()  # Split line by tab separator\n",
    "        if len(parts) >= 4:  # Check if we have at least 4 parts\n",
    "            query_id = parts[0]  # First column\n",
    "            doc_id = parts[2]    # Third column\n",
    "            list_qrels.append([query_id, doc_id])  # Append [query_id, doc_id] to list_qrels\n",
    "        else:\n",
    "            print(f\"Warning: Skipping line with unexpected format: {line}\")\n",
    "            print(f\"The were so many part: {len(parts)}\")\n",
    "\n",
    "# Create a DataFrame from list_qrels with columns 'query_id' and 'doc_id'\n",
    "df_qrels = pd.DataFrame(list_qrels, columns=['query_id', 'doc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "batch_embedder_eval = BatchWordEmbedder(word_embedder, device, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_tuple_reader_eval = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=512, max_query_length=512)\n",
    "#Load modified tuples\n",
    "_tuple_reader_eval = _tuple_reader_eval.read(config['validation_data'])\n",
    "_tuple_reader_eval.index_with(vocab)\n",
    "loader_test = PyTorchDataLoader(_tuple_reader_eval, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, df_qrels, loader, batch_embedder, device):\n",
    "    model.eval()\n",
    "    query_ids = []\n",
    "    doc_ids = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(loader):\n",
    "            try:\n",
    "                query_emb, doc_pos_emb, _, query_pad_mask, document_pad_mask_pos, _ = batch_embedder(batch)\n",
    "\n",
    "                pred = model(query_emb, doc_pos_emb, query_pad_mask, document_pad_mask_pos)\n",
    "                query_ids.extend(batch['query_id'])  # Directly extend the list\n",
    "                doc_ids.extend(batch['doc_id'])      # Directly extend the list\n",
    "                preds.extend(pred.cpu().numpy().flatten())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {idx}: {e}\")\n",
    "                print(\"Skipping batch with query_id and doc_id:\")\n",
    "                print(\"query_id:\", batch['query_id'])\n",
    "                print(\"doc_id:\", batch['doc_id'])\n",
    "                continue\n",
    "            # if idx * 32 >= 10000:\n",
    "            #     break\n",
    "    print(\"Finished!\")\n",
    "    df_eval = pd.DataFrame({\n",
    "        'query_id': query_ids,\n",
    "        'doc_id': doc_ids,\n",
    "        'score': preds\n",
    "    })\n",
    "    \n",
    "    # Assigning rank based on scores within each query_id group\n",
    "    df_eval['rank'] = df_eval.groupby('query_id')['score'].rank(ascending=False, method='first').astype(int)\n",
    "    \n",
    "    # Sorting by query_id and rank\n",
    "    df_eval = df_eval.sort_values(by=['query_id', 'rank'])\n",
    "    \n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d562ca0ef94143e8b42b508eff7cb56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "reading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "df_eval = evaluate_model(model_knrm, df_qrels, loader_test, batch_embedder_eval, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Write the DataFrame to a TSV file\n",
    "path_result= os.path.join(base_path, \"data/results_part2/knrm_fira_baseline_final.tsv\")\n",
    "path_baseline=config['qrels']\n",
    "#Write this to the results\n",
    "df_eval.to_csv(path_result, sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR@10': 0.9560344827586207,\n",
       " 'Recall@10': 0.9508765283703707,\n",
       " 'QueriesWithNoRelevant@10': 115,\n",
       " 'QueriesWithRelevant@10': 4060,\n",
       " 'AverageRankGoldLabel@10': 1.1029556650246306,\n",
       " 'MedianRankGoldLabel@10': 1.0,\n",
       " 'MRR@20': 0.9560344827586207,\n",
       " 'Recall@20': 1.0000527797325827,\n",
       " 'QueriesWithNoRelevant@20': 115,\n",
       " 'QueriesWithRelevant@20': 4060,\n",
       " 'AverageRankGoldLabel@20': 1.1029556650246306,\n",
       " 'MedianRankGoldLabel@20': 1.0,\n",
       " 'MRR@1000': 0.9560344827586207,\n",
       " 'Recall@1000': 1.0000527797325827,\n",
       " 'QueriesWithNoRelevant@1000': 115,\n",
       " 'QueriesWithRelevant@1000': 4060,\n",
       " 'AverageRankGoldLabel@1000': 1.1029556650246306,\n",
       " 'MedianRankGoldLabel@1000': 1.0,\n",
       " 'nDCG@3': 0.870678805114396,\n",
       " 'nDCG@5': 0.8755932545053234,\n",
       " 'nDCG@10': 0.9001940416552495,\n",
       " 'nDCG@20': 0.9139540272527631,\n",
       " 'nDCG@1000': 0.9139540272527631,\n",
       " 'QueriesRanked': 4060,\n",
       " 'MAP@1000': 0.9468693220351503}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.core_metrics import calculate_metrics_plain,load_ranking,load_qrels\n",
    "\n",
    "calculate_metrics_plain(load_ranking(path_result),load_qrels(path_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**FIRA our own judgement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MRR@10': 0.9477214289621971,\n",
       " 'Recall@10': 0.9517564585696048,\n",
       " 'QueriesWithNoRelevant@10': 113,\n",
       " 'QueriesWithRelevant@10': 4062,\n",
       " 'AverageRankGoldLabel@10': 1.1233382570162482,\n",
       " 'MedianRankGoldLabel@10': 1.0,\n",
       " 'MRR@20': 0.9477214289621971,\n",
       " 'Recall@20': 1.000052753745516,\n",
       " 'QueriesWithNoRelevant@20': 113,\n",
       " 'QueriesWithRelevant@20': 4062,\n",
       " 'AverageRankGoldLabel@20': 1.1233382570162482,\n",
       " 'MedianRankGoldLabel@20': 1.0,\n",
       " 'MRR@1000': 0.9477214289621971,\n",
       " 'Recall@1000': 1.000052753745516,\n",
       " 'QueriesWithNoRelevant@1000': 113,\n",
       " 'QueriesWithRelevant@1000': 4062,\n",
       " 'AverageRankGoldLabel@1000': 1.1233382570162482,\n",
       " 'MedianRankGoldLabel@1000': 1.0,\n",
       " 'nDCG@3': 0.8614270150469169,\n",
       " 'nDCG@5': 0.8688532389398897,\n",
       " 'nDCG@10': 0.8945172252825958,\n",
       " 'nDCG@20': 0.9077459409980746,\n",
       " 'nDCG@1000': 0.9077459409980746,\n",
       " 'QueriesRanked': 4062,\n",
       " 'MAP@1000': 0.9362002840850584}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_baseline_own=os.path.join(base_path,\"data/Part-1/fira-22.judgements-anonymized-aggregated_v1.tsv\")\n",
    "calculate_metrics_plain(load_ranking(path_result),load_qrels(path_baseline_own))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "air_ex2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
